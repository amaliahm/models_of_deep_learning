{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolutional ResNet and Residual Blocks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([128, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ResNet with identity blocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 1st residual block\n",
    "\n",
    "        # 28x28x1 => 28x28x4\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=1,\n",
    "                                      out_channels=4,\n",
    "                                      kernel_size=(1, 1),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=0)\n",
    "        self.conv_1_bn = torch.nn.BatchNorm2d(4)\n",
    "                                    \n",
    "        # 28x28x4 => 28x28x1\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=4,\n",
    "                                      out_channels=1,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1)   \n",
    "        self.conv_2_bn = torch.nn.BatchNorm2d(1)\n",
    "        \n",
    "        # 2nd residual block\n",
    "\n",
    "        # 28x28x1 => 28x28x4\n",
    "        self.conv_3 = torch.nn.Conv2d(in_channels=1,\n",
    "                                      out_channels=4,\n",
    "                                      kernel_size=(1, 1),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=0)\n",
    "        self.conv_3_bn = torch.nn.BatchNorm2d(4)\n",
    "                                    \n",
    "        # 28x28x4 => 28x28x1\n",
    "        self.conv_4 = torch.nn.Conv2d(in_channels=4,\n",
    "                                      out_channels=1,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1)   \n",
    "        self.conv_4_bn = torch.nn.BatchNorm2d(1)\n",
    "\n",
    "        # Fully connected\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(28*28*1, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1st residual block\n",
    "\n",
    "        shortcut = x\n",
    "        \n",
    "        out = self.conv_1(x)\n",
    "        out = self.conv_1_bn(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv_2(out)\n",
    "        out = self.conv_2_bn(out)\n",
    "        \n",
    "        out += shortcut\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # 2nd residual block\n",
    "        \n",
    "        shortcut = out\n",
    "        \n",
    "        out = self.conv_3(out)\n",
    "        out = self.conv_3_bn(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv_4(out)\n",
    "        out = self.conv_4_bn(out)\n",
    "        \n",
    "        out += shortcut\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Fully connected\n",
    "\n",
    "        logits = self.linear_1(out.view(-1, 28*28*1))\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "model = ConvNet(num_classes=num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/469 | Cost: 2.6800\n",
      "Epoch: 001/010 | Batch 050/469 | Cost: 0.2571\n",
      "Epoch: 001/010 | Batch 100/469 | Cost: 0.3645\n",
      "Epoch: 001/010 | Batch 150/469 | Cost: 0.2635\n",
      "Epoch: 001/010 | Batch 200/469 | Cost: 0.4046\n",
      "Epoch: 001/010 | Batch 250/469 | Cost: 0.3150\n",
      "Epoch: 001/010 | Batch 300/469 | Cost: 0.3945\n",
      "Epoch: 001/010 | Batch 350/469 | Cost: 0.2483\n",
      "Epoch: 001/010 | Batch 400/469 | Cost: 0.2535\n",
      "Epoch: 001/010 | Batch 450/469 | Cost: 0.3532\n",
      "Epoch: 001/010 training accuracy: 90.93%\n",
      "Time elapsed: 2.06 min\n",
      "Epoch: 002/010 | Batch 000/469 | Cost: 0.3232\n",
      "Epoch: 002/010 | Batch 050/469 | Cost: 0.2191\n",
      "Epoch: 002/010 | Batch 100/469 | Cost: 0.3152\n",
      "Epoch: 002/010 | Batch 150/469 | Cost: 0.2102\n",
      "Epoch: 002/010 | Batch 200/469 | Cost: 0.3202\n",
      "Epoch: 002/010 | Batch 250/469 | Cost: 0.2112\n",
      "Epoch: 002/010 | Batch 300/469 | Cost: 0.2701\n",
      "Epoch: 002/010 | Batch 350/469 | Cost: 0.4034\n",
      "Epoch: 002/010 | Batch 400/469 | Cost: 0.2849\n",
      "Epoch: 002/010 | Batch 450/469 | Cost: 0.2930\n",
      "Epoch: 002/010 training accuracy: 92.40%\n",
      "Time elapsed: 4.16 min\n",
      "Epoch: 003/010 | Batch 000/469 | Cost: 0.1997\n",
      "Epoch: 003/010 | Batch 050/469 | Cost: 0.4804\n",
      "Epoch: 003/010 | Batch 100/469 | Cost: 0.2017\n",
      "Epoch: 003/010 | Batch 150/469 | Cost: 0.2491\n",
      "Epoch: 003/010 | Batch 200/469 | Cost: 0.2790\n",
      "Epoch: 003/010 | Batch 250/469 | Cost: 0.2814\n",
      "Epoch: 003/010 | Batch 300/469 | Cost: 0.3110\n",
      "Epoch: 003/010 | Batch 350/469 | Cost: 0.3342\n",
      "Epoch: 003/010 | Batch 400/469 | Cost: 0.3418\n",
      "Epoch: 003/010 | Batch 450/469 | Cost: 0.3079\n",
      "Epoch: 003/010 training accuracy: 92.44%\n",
      "Time elapsed: 1445.51 min\n",
      "Epoch: 004/010 | Batch 000/469 | Cost: 0.2764\n",
      "Epoch: 004/010 | Batch 050/469 | Cost: 0.1210\n",
      "Epoch: 004/010 | Batch 100/469 | Cost: 0.3749\n",
      "Epoch: 004/010 | Batch 150/469 | Cost: 0.3286\n",
      "Epoch: 004/010 | Batch 200/469 | Cost: 0.3563\n",
      "Epoch: 004/010 | Batch 250/469 | Cost: 0.3025\n",
      "Epoch: 004/010 | Batch 300/469 | Cost: 0.4611\n",
      "Epoch: 004/010 | Batch 350/469 | Cost: 0.1987\n",
      "Epoch: 004/010 | Batch 400/469 | Cost: 0.2531\n",
      "Epoch: 004/010 | Batch 450/469 | Cost: 0.2440\n",
      "Epoch: 004/010 training accuracy: 92.93%\n",
      "Time elapsed: 1446.85 min\n",
      "Epoch: 005/010 | Batch 000/469 | Cost: 0.3618\n",
      "Epoch: 005/010 | Batch 050/469 | Cost: 0.2425\n",
      "Epoch: 005/010 | Batch 100/469 | Cost: 0.0774\n",
      "Epoch: 005/010 | Batch 150/469 | Cost: 0.2133\n",
      "Epoch: 005/010 | Batch 200/469 | Cost: 0.1349\n",
      "Epoch: 005/010 | Batch 250/469 | Cost: 0.2850\n",
      "Epoch: 005/010 | Batch 300/469 | Cost: 0.2887\n",
      "Epoch: 005/010 | Batch 350/469 | Cost: 0.2794\n",
      "Epoch: 005/010 | Batch 400/469 | Cost: 0.1669\n",
      "Epoch: 005/010 | Batch 450/469 | Cost: 0.3569\n",
      "Epoch: 005/010 training accuracy: 93.31%\n",
      "Time elapsed: 1448.19 min\n",
      "Epoch: 006/010 | Batch 000/469 | Cost: 0.2588\n",
      "Epoch: 006/010 | Batch 050/469 | Cost: 0.1501\n",
      "Epoch: 006/010 | Batch 100/469 | Cost: 0.1658\n",
      "Epoch: 006/010 | Batch 150/469 | Cost: 0.1008\n",
      "Epoch: 006/010 | Batch 200/469 | Cost: 0.2832\n",
      "Epoch: 006/010 | Batch 250/469 | Cost: 0.2461\n",
      "Epoch: 006/010 | Batch 300/469 | Cost: 0.3543\n",
      "Epoch: 006/010 | Batch 350/469 | Cost: 0.2488\n",
      "Epoch: 006/010 | Batch 400/469 | Cost: 0.2389\n",
      "Epoch: 006/010 | Batch 450/469 | Cost: 0.2276\n",
      "Epoch: 006/010 training accuracy: 90.68%\n",
      "Time elapsed: 1449.41 min\n",
      "Epoch: 007/010 | Batch 000/469 | Cost: 0.2099\n",
      "Epoch: 007/010 | Batch 050/469 | Cost: 0.3219\n",
      "Epoch: 007/010 | Batch 100/469 | Cost: 0.2071\n",
      "Epoch: 007/010 | Batch 150/469 | Cost: 0.1798\n",
      "Epoch: 007/010 | Batch 200/469 | Cost: 0.1971\n",
      "Epoch: 007/010 | Batch 250/469 | Cost: 0.1634\n",
      "Epoch: 007/010 | Batch 300/469 | Cost: 0.1995\n",
      "Epoch: 007/010 | Batch 350/469 | Cost: 0.2487\n",
      "Epoch: 007/010 | Batch 400/469 | Cost: 0.2443\n",
      "Epoch: 007/010 | Batch 450/469 | Cost: 0.1744\n",
      "Epoch: 007/010 training accuracy: 93.50%\n",
      "Time elapsed: 1450.76 min\n",
      "Epoch: 008/010 | Batch 000/469 | Cost: 0.3893\n",
      "Epoch: 008/010 | Batch 050/469 | Cost: 0.1327\n",
      "Epoch: 008/010 | Batch 100/469 | Cost: 0.1439\n",
      "Epoch: 008/010 | Batch 150/469 | Cost: 0.2147\n",
      "Epoch: 008/010 | Batch 200/469 | Cost: 0.1779\n",
      "Epoch: 008/010 | Batch 250/469 | Cost: 0.3007\n",
      "Epoch: 008/010 | Batch 300/469 | Cost: 0.2697\n",
      "Epoch: 008/010 | Batch 350/469 | Cost: 0.2821\n",
      "Epoch: 008/010 | Batch 400/469 | Cost: 0.3464\n",
      "Epoch: 008/010 | Batch 450/469 | Cost: 0.2315\n",
      "Epoch: 008/010 training accuracy: 92.71%\n",
      "Time elapsed: 1452.02 min\n",
      "Epoch: 009/010 | Batch 000/469 | Cost: 0.2000\n",
      "Epoch: 009/010 | Batch 050/469 | Cost: 0.2613\n",
      "Epoch: 009/010 | Batch 100/469 | Cost: 0.2299\n",
      "Epoch: 009/010 | Batch 150/469 | Cost: 0.2089\n",
      "Epoch: 009/010 | Batch 200/469 | Cost: 0.2053\n",
      "Epoch: 009/010 | Batch 250/469 | Cost: 0.1763\n",
      "Epoch: 009/010 | Batch 300/469 | Cost: 0.2479\n",
      "Epoch: 009/010 | Batch 350/469 | Cost: 0.1949\n",
      "Epoch: 009/010 | Batch 400/469 | Cost: 0.1928\n",
      "Epoch: 009/010 | Batch 450/469 | Cost: 0.2385\n",
      "Epoch: 009/010 training accuracy: 93.90%\n",
      "Time elapsed: 1453.30 min\n",
      "Epoch: 010/010 | Batch 000/469 | Cost: 0.1594\n",
      "Epoch: 010/010 | Batch 050/469 | Cost: 0.2410\n",
      "Epoch: 010/010 | Batch 100/469 | Cost: 0.2087\n",
      "Epoch: 010/010 | Batch 150/469 | Cost: 0.1434\n",
      "Epoch: 010/010 | Batch 200/469 | Cost: 0.1944\n",
      "Epoch: 010/010 | Batch 250/469 | Cost: 0.3130\n",
      "Epoch: 010/010 | Batch 300/469 | Cost: 0.1849\n",
      "Epoch: 010/010 | Batch 350/469 | Cost: 0.2646\n",
      "Epoch: 010/010 | Batch 400/469 | Cost: 0.2078\n",
      "Epoch: 010/010 | Batch 450/469 | Cost: 0.2398\n",
      "Epoch: 010/010 training accuracy: 93.42%\n",
      "Time elapsed: 1454.57 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "    model = model.eval() \n",
    "    with torch.set_grad_enabled(False): \n",
    "        print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "              epoch+1, num_epochs, \n",
    "              compute_accuracy(model, train_loader)))\n",
    "\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time: 1454.57 min\n"
     ]
    }
   ],
   "source": [
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 92.11%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ResNet with convolutional blocks for resizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # 1st residual block\n",
    "\n",
    "        # 28x28x1 => 14x14x4 \n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=1,\n",
    "                                      out_channels=4,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(2, 2),\n",
    "                                      padding=1)\n",
    "        self.conv_1_bn = torch.nn.BatchNorm2d(4)\n",
    "                                    \n",
    "        # 14x14x4 => 14x14x8\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=4,\n",
    "                                      out_channels=8,\n",
    "                                      kernel_size=(1, 1),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=0)   \n",
    "        self.conv_2_bn = torch.nn.BatchNorm2d(8)\n",
    "        \n",
    "        # 28x28x1 => 14x14x8\n",
    "        self.conv_shortcut_1 = torch.nn.Conv2d(in_channels=1,\n",
    "                                               out_channels=8,\n",
    "                                               kernel_size=(1, 1),\n",
    "                                               stride=(2, 2),\n",
    "                                               padding=0)   \n",
    "        self.conv_shortcut_1_bn = torch.nn.BatchNorm2d(8)\n",
    "        \n",
    "        # 2nd residual block\n",
    "\n",
    "        # 14x14x8 => 7x7x16 \n",
    "        self.conv_3 = torch.nn.Conv2d(in_channels=8,\n",
    "                                      out_channels=16,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(2, 2),\n",
    "                                      padding=1)\n",
    "        self.conv_3_bn = torch.nn.BatchNorm2d(16)\n",
    "                                    \n",
    "        # 7x7x16 => 7x7x32\n",
    "        self.conv_4 = torch.nn.Conv2d(in_channels=16,\n",
    "                                      out_channels=32,\n",
    "                                      kernel_size=(1, 1),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=0)   \n",
    "        self.conv_4_bn = torch.nn.BatchNorm2d(32)\n",
    "        \n",
    "        # 14x14x8 => 7x7x32 \n",
    "        self.conv_shortcut_2 = torch.nn.Conv2d(in_channels=8,\n",
    "                                               out_channels=32,\n",
    "                                               kernel_size=(1, 1),\n",
    "                                               stride=(2, 2),\n",
    "                                               padding=0)   \n",
    "        self.conv_shortcut_2_bn = torch.nn.BatchNorm2d(32)\n",
    "\n",
    "        # Fully connected    \n",
    "        self.linear_1 = torch.nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # 1st residual block\n",
    "    \n",
    "        shortcut = x\n",
    "        \n",
    "        out = self.conv_1(x) # 28x28x1 => 14x14x4 \n",
    "        out = self.conv_1_bn(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv_2(out) # 14x14x4 => 714x14x8\n",
    "        out = self.conv_2_bn(out)\n",
    "        \n",
    "        # match up dimensions using a linear function (no relu)\n",
    "        shortcut = self.conv_shortcut_1(shortcut)\n",
    "        shortcut = self.conv_shortcut_1_bn(shortcut)\n",
    "        \n",
    "        out += shortcut\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # 2nd residual block\n",
    "        \n",
    "        shortcut = out\n",
    "        \n",
    "        out = self.conv_3(out) # 14x14x8 => 7x7x16 \n",
    "        out = self.conv_3_bn(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv_4(out) # 7x7x16 => 7x7x32\n",
    "        out = self.conv_4_bn(out)\n",
    "        \n",
    "        # match up dimensions using a linear function (no relu)\n",
    "        shortcut = self.conv_shortcut_2(shortcut)\n",
    "        shortcut = self.conv_shortcut_2_bn(shortcut)\n",
    "        \n",
    "        out += shortcut\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Fully connected\n",
    "        \n",
    "        logits = self.linear_1(out.view(-1, 7*7*32))\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "model = ConvNet(num_classes=num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/469 | Cost: 2.3534\n",
      "Epoch: 001/010 | Batch 050/469 | Cost: 0.2685\n",
      "Epoch: 001/010 | Batch 100/469 | Cost: 0.2464\n",
      "Epoch: 001/010 | Batch 150/469 | Cost: 0.0995\n",
      "Epoch: 001/010 | Batch 200/469 | Cost: 0.0619\n",
      "Epoch: 001/010 | Batch 250/469 | Cost: 0.1123\n",
      "Epoch: 001/010 | Batch 300/469 | Cost: 0.2530\n",
      "Epoch: 001/010 | Batch 350/469 | Cost: 0.1477\n",
      "Epoch: 001/010 | Batch 400/469 | Cost: 0.0631\n",
      "Epoch: 001/010 | Batch 450/469 | Cost: 0.1083\n",
      "Epoch: 001/010 training accuracy: 97.53%\n",
      "Epoch: 002/010 | Batch 000/469 | Cost: 0.1181\n",
      "Epoch: 002/010 | Batch 050/469 | Cost: 0.0374\n",
      "Epoch: 002/010 | Batch 100/469 | Cost: 0.1096\n",
      "Epoch: 002/010 | Batch 150/469 | Cost: 0.1729\n",
      "Epoch: 002/010 | Batch 200/469 | Cost: 0.1072\n",
      "Epoch: 002/010 | Batch 250/469 | Cost: 0.0343\n",
      "Epoch: 002/010 | Batch 300/469 | Cost: 0.0258\n",
      "Epoch: 002/010 | Batch 350/469 | Cost: 0.0444\n",
      "Epoch: 002/010 | Batch 400/469 | Cost: 0.0288\n",
      "Epoch: 002/010 | Batch 450/469 | Cost: 0.1070\n",
      "Epoch: 002/010 training accuracy: 98.42%\n",
      "Epoch: 003/010 | Batch 000/469 | Cost: 0.0355\n",
      "Epoch: 003/010 | Batch 050/469 | Cost: 0.0608\n",
      "Epoch: 003/010 | Batch 100/469 | Cost: 0.1474\n",
      "Epoch: 003/010 | Batch 150/469 | Cost: 0.0140\n",
      "Epoch: 003/010 | Batch 200/469 | Cost: 0.0429\n",
      "Epoch: 003/010 | Batch 250/469 | Cost: 0.0301\n",
      "Epoch: 003/010 | Batch 300/469 | Cost: 0.0241\n",
      "Epoch: 003/010 | Batch 350/469 | Cost: 0.0115\n",
      "Epoch: 003/010 | Batch 400/469 | Cost: 0.0664\n",
      "Epoch: 003/010 | Batch 450/469 | Cost: 0.0170\n",
      "Epoch: 003/010 training accuracy: 98.72%\n",
      "Epoch: 004/010 | Batch 000/469 | Cost: 0.0270\n",
      "Epoch: 004/010 | Batch 050/469 | Cost: 0.0320\n",
      "Epoch: 004/010 | Batch 100/469 | Cost: 0.0469\n",
      "Epoch: 004/010 | Batch 150/469 | Cost: 0.0882\n",
      "Epoch: 004/010 | Batch 200/469 | Cost: 0.0477\n",
      "Epoch: 004/010 | Batch 250/469 | Cost: 0.0261\n",
      "Epoch: 004/010 | Batch 300/469 | Cost: 0.0670\n",
      "Epoch: 004/010 | Batch 350/469 | Cost: 0.0522\n",
      "Epoch: 004/010 | Batch 400/469 | Cost: 0.0453\n",
      "Epoch: 004/010 | Batch 450/469 | Cost: 0.0189\n",
      "Epoch: 004/010 training accuracy: 98.67%\n",
      "Epoch: 005/010 | Batch 000/469 | Cost: 0.0416\n",
      "Epoch: 005/010 | Batch 050/469 | Cost: 0.0220\n",
      "Epoch: 005/010 | Batch 100/469 | Cost: 0.0985\n",
      "Epoch: 005/010 | Batch 150/469 | Cost: 0.0533\n",
      "Epoch: 005/010 | Batch 200/469 | Cost: 0.0673\n",
      "Epoch: 005/010 | Batch 250/469 | Cost: 0.0495\n",
      "Epoch: 005/010 | Batch 300/469 | Cost: 0.0133\n",
      "Epoch: 005/010 | Batch 350/469 | Cost: 0.0149\n",
      "Epoch: 005/010 | Batch 400/469 | Cost: 0.0101\n",
      "Epoch: 005/010 | Batch 450/469 | Cost: 0.0562\n",
      "Epoch: 005/010 training accuracy: 98.83%\n",
      "Epoch: 006/010 | Batch 000/469 | Cost: 0.0717\n",
      "Epoch: 006/010 | Batch 050/469 | Cost: 0.0105\n",
      "Epoch: 006/010 | Batch 100/469 | Cost: 0.0071\n",
      "Epoch: 006/010 | Batch 150/469 | Cost: 0.0500\n",
      "Epoch: 006/010 | Batch 200/469 | Cost: 0.0223\n",
      "Epoch: 006/010 | Batch 250/469 | Cost: 0.0735\n",
      "Epoch: 006/010 | Batch 300/469 | Cost: 0.0091\n",
      "Epoch: 006/010 | Batch 350/469 | Cost: 0.0224\n",
      "Epoch: 006/010 | Batch 400/469 | Cost: 0.0241\n",
      "Epoch: 006/010 | Batch 450/469 | Cost: 0.0599\n",
      "Epoch: 006/010 training accuracy: 99.03%\n",
      "Epoch: 007/010 | Batch 000/469 | Cost: 0.0265\n",
      "Epoch: 007/010 | Batch 050/469 | Cost: 0.0428\n",
      "Epoch: 007/010 | Batch 100/469 | Cost: 0.0146\n",
      "Epoch: 007/010 | Batch 150/469 | Cost: 0.0185\n",
      "Epoch: 007/010 | Batch 200/469 | Cost: 0.0183\n",
      "Epoch: 007/010 | Batch 250/469 | Cost: 0.0107\n",
      "Epoch: 007/010 | Batch 300/469 | Cost: 0.0715\n",
      "Epoch: 007/010 | Batch 350/469 | Cost: 0.1862\n",
      "Epoch: 007/010 | Batch 400/469 | Cost: 0.0123\n",
      "Epoch: 007/010 | Batch 450/469 | Cost: 0.0277\n",
      "Epoch: 007/010 training accuracy: 99.04%\n",
      "Epoch: 008/010 | Batch 000/469 | Cost: 0.0241\n",
      "Epoch: 008/010 | Batch 050/469 | Cost: 0.0840\n",
      "Epoch: 008/010 | Batch 100/469 | Cost: 0.0183\n",
      "Epoch: 008/010 | Batch 150/469 | Cost: 0.0223\n",
      "Epoch: 008/010 | Batch 200/469 | Cost: 0.1027\n",
      "Epoch: 008/010 | Batch 250/469 | Cost: 0.0027\n",
      "Epoch: 008/010 | Batch 300/469 | Cost: 0.0147\n",
      "Epoch: 008/010 | Batch 350/469 | Cost: 0.0425\n",
      "Epoch: 008/010 | Batch 400/469 | Cost: 0.0126\n",
      "Epoch: 008/010 | Batch 450/469 | Cost: 0.0262\n",
      "Epoch: 008/010 training accuracy: 99.18%\n",
      "Epoch: 009/010 | Batch 000/469 | Cost: 0.0091\n",
      "Epoch: 009/010 | Batch 050/469 | Cost: 0.0151\n",
      "Epoch: 009/010 | Batch 100/469 | Cost: 0.0099\n",
      "Epoch: 009/010 | Batch 150/469 | Cost: 0.0467\n",
      "Epoch: 009/010 | Batch 200/469 | Cost: 0.0069\n",
      "Epoch: 009/010 | Batch 250/469 | Cost: 0.0483\n",
      "Epoch: 009/010 | Batch 300/469 | Cost: 0.0719\n",
      "Epoch: 009/010 | Batch 350/469 | Cost: 0.0298\n",
      "Epoch: 009/010 | Batch 400/469 | Cost: 0.0363\n",
      "Epoch: 009/010 | Batch 450/469 | Cost: 0.0081\n",
      "Epoch: 009/010 training accuracy: 99.39%\n",
      "Epoch: 010/010 | Batch 000/469 | Cost: 0.0338\n",
      "Epoch: 010/010 | Batch 050/469 | Cost: 0.0038\n",
      "Epoch: 010/010 | Batch 100/469 | Cost: 0.0226\n",
      "Epoch: 010/010 | Batch 150/469 | Cost: 0.0187\n",
      "Epoch: 010/010 | Batch 200/469 | Cost: 0.0104\n",
      "Epoch: 010/010 | Batch 250/469 | Cost: 0.0092\n",
      "Epoch: 010/010 | Batch 300/469 | Cost: 0.0819\n",
      "Epoch: 010/010 | Batch 350/469 | Cost: 0.0491\n",
      "Epoch: 010/010 | Batch 400/469 | Cost: 0.0549\n",
      "Epoch: 010/010 | Batch 450/469 | Cost: 0.0020\n",
      "Epoch: 010/010 training accuracy: 99.24%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "            \n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "    model = model.eval() # eval mode to prevent upd. batchnorm params during inference\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "              epoch+1, num_epochs, \n",
    "              compute_accuracy(model, train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.33%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ResNet with convolutional blocks for resizing (using a helper class)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        \n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=channels[0],\n",
    "                                      out_channels=channels[1],\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(2, 2),\n",
    "                                      padding=1)\n",
    "        self.conv_1_bn = torch.nn.BatchNorm2d(channels[1])\n",
    "                                    \n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=channels[1],\n",
    "                                      out_channels=channels[2],\n",
    "                                      kernel_size=(1, 1),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=0)   \n",
    "        self.conv_2_bn = torch.nn.BatchNorm2d(channels[2])\n",
    "\n",
    "        self.conv_shortcut_1 = torch.nn.Conv2d(in_channels=channels[0],\n",
    "                                               out_channels=channels[2],\n",
    "                                               kernel_size=(1, 1),\n",
    "                                               stride=(2, 2),\n",
    "                                               padding=0)   \n",
    "        self.conv_shortcut_1_bn = torch.nn.BatchNorm2d(channels[2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        \n",
    "        out = self.conv_1(x)\n",
    "        out = self.conv_1_bn(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv_2(out)\n",
    "        out = self.conv_2_bn(out)\n",
    "        \n",
    "        shortcut = self.conv_shortcut_1(shortcut)\n",
    "        shortcut = self.conv_shortcut_1_bn(shortcut)\n",
    "        \n",
    "        out += shortcut\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.residual_block_1 = ResidualBlock(channels=[1, 4, 8])\n",
    "        self.residual_block_2 = ResidualBlock(channels=[8, 16, 32])\n",
    "    \n",
    "        self.linear_1 = torch.nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.residual_block_1.forward(x)\n",
    "        out = self.residual_block_2.forward(out)\n",
    "         \n",
    "        logits = self.linear_1(out.view(-1, 7*7*32))\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "model = ConvNet(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/468 | Cost: 2.3534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 050/468 | Cost: 0.2685\n",
      "Epoch: 001/010 | Batch 100/468 | Cost: 0.2464\n",
      "Epoch: 001/010 | Batch 150/468 | Cost: 0.0995\n",
      "Epoch: 001/010 | Batch 200/468 | Cost: 0.0619\n",
      "Epoch: 001/010 | Batch 250/468 | Cost: 0.1123\n",
      "Epoch: 001/010 | Batch 300/468 | Cost: 0.2530\n",
      "Epoch: 001/010 | Batch 350/468 | Cost: 0.1477\n",
      "Epoch: 001/010 | Batch 400/468 | Cost: 0.0631\n",
      "Epoch: 001/010 | Batch 450/468 | Cost: 0.1083\n",
      "Epoch: 001/010 training accuracy: 97.53%\n",
      "Epoch: 002/010 | Batch 000/468 | Cost: 0.1181\n",
      "Epoch: 002/010 | Batch 050/468 | Cost: 0.0374\n",
      "Epoch: 002/010 | Batch 100/468 | Cost: 0.1096\n",
      "Epoch: 002/010 | Batch 150/468 | Cost: 0.1729\n",
      "Epoch: 002/010 | Batch 200/468 | Cost: 0.1072\n",
      "Epoch: 002/010 | Batch 250/468 | Cost: 0.0343\n",
      "Epoch: 002/010 | Batch 300/468 | Cost: 0.0258\n",
      "Epoch: 002/010 | Batch 350/468 | Cost: 0.0444\n",
      "Epoch: 002/010 | Batch 400/468 | Cost: 0.0288\n",
      "Epoch: 002/010 | Batch 450/468 | Cost: 0.1070\n",
      "Epoch: 002/010 training accuracy: 98.42%\n",
      "Epoch: 003/010 | Batch 000/468 | Cost: 0.0355\n",
      "Epoch: 003/010 | Batch 050/468 | Cost: 0.0608\n",
      "Epoch: 003/010 | Batch 100/468 | Cost: 0.1474\n",
      "Epoch: 003/010 | Batch 150/468 | Cost: 0.0140\n",
      "Epoch: 003/010 | Batch 200/468 | Cost: 0.0429\n",
      "Epoch: 003/010 | Batch 250/468 | Cost: 0.0301\n",
      "Epoch: 003/010 | Batch 300/468 | Cost: 0.0241\n",
      "Epoch: 003/010 | Batch 350/468 | Cost: 0.0115\n",
      "Epoch: 003/010 | Batch 400/468 | Cost: 0.0664\n",
      "Epoch: 003/010 | Batch 450/468 | Cost: 0.0170\n",
      "Epoch: 003/010 training accuracy: 98.72%\n",
      "Epoch: 004/010 | Batch 000/468 | Cost: 0.0270\n",
      "Epoch: 004/010 | Batch 050/468 | Cost: 0.0320\n",
      "Epoch: 004/010 | Batch 100/468 | Cost: 0.0469\n",
      "Epoch: 004/010 | Batch 150/468 | Cost: 0.0882\n",
      "Epoch: 004/010 | Batch 200/468 | Cost: 0.0477\n",
      "Epoch: 004/010 | Batch 250/468 | Cost: 0.0261\n",
      "Epoch: 004/010 | Batch 300/468 | Cost: 0.0670\n",
      "Epoch: 004/010 | Batch 350/468 | Cost: 0.0522\n",
      "Epoch: 004/010 | Batch 400/468 | Cost: 0.0453\n",
      "Epoch: 004/010 | Batch 450/468 | Cost: 0.0189\n",
      "Epoch: 004/010 training accuracy: 98.67%\n",
      "Epoch: 005/010 | Batch 000/468 | Cost: 0.0416\n",
      "Epoch: 005/010 | Batch 050/468 | Cost: 0.0220\n",
      "Epoch: 005/010 | Batch 100/468 | Cost: 0.0985\n",
      "Epoch: 005/010 | Batch 150/468 | Cost: 0.0533\n",
      "Epoch: 005/010 | Batch 200/468 | Cost: 0.0673\n",
      "Epoch: 005/010 | Batch 250/468 | Cost: 0.0495\n",
      "Epoch: 005/010 | Batch 300/468 | Cost: 0.0133\n",
      "Epoch: 005/010 | Batch 350/468 | Cost: 0.0149\n",
      "Epoch: 005/010 | Batch 400/468 | Cost: 0.0101\n",
      "Epoch: 005/010 | Batch 450/468 | Cost: 0.0562\n",
      "Epoch: 005/010 training accuracy: 98.83%\n",
      "Epoch: 006/010 | Batch 000/468 | Cost: 0.0717\n",
      "Epoch: 006/010 | Batch 050/468 | Cost: 0.0105\n",
      "Epoch: 006/010 | Batch 100/468 | Cost: 0.0071\n",
      "Epoch: 006/010 | Batch 150/468 | Cost: 0.0500\n",
      "Epoch: 006/010 | Batch 200/468 | Cost: 0.0223\n",
      "Epoch: 006/010 | Batch 250/468 | Cost: 0.0735\n",
      "Epoch: 006/010 | Batch 300/468 | Cost: 0.0091\n",
      "Epoch: 006/010 | Batch 350/468 | Cost: 0.0224\n",
      "Epoch: 006/010 | Batch 400/468 | Cost: 0.0241\n",
      "Epoch: 006/010 | Batch 450/468 | Cost: 0.0599\n",
      "Epoch: 006/010 training accuracy: 99.03%\n",
      "Epoch: 007/010 | Batch 000/468 | Cost: 0.0265\n",
      "Epoch: 007/010 | Batch 050/468 | Cost: 0.0428\n",
      "Epoch: 007/010 | Batch 100/468 | Cost: 0.0146\n",
      "Epoch: 007/010 | Batch 150/468 | Cost: 0.0185\n",
      "Epoch: 007/010 | Batch 200/468 | Cost: 0.0183\n",
      "Epoch: 007/010 | Batch 250/468 | Cost: 0.0107\n",
      "Epoch: 007/010 | Batch 300/468 | Cost: 0.0715\n",
      "Epoch: 007/010 | Batch 350/468 | Cost: 0.1862\n",
      "Epoch: 007/010 | Batch 400/468 | Cost: 0.0123\n",
      "Epoch: 007/010 | Batch 450/468 | Cost: 0.0277\n",
      "Epoch: 007/010 training accuracy: 99.04%\n",
      "Epoch: 008/010 | Batch 000/468 | Cost: 0.0241\n",
      "Epoch: 008/010 | Batch 050/468 | Cost: 0.0840\n",
      "Epoch: 008/010 | Batch 100/468 | Cost: 0.0183\n",
      "Epoch: 008/010 | Batch 150/468 | Cost: 0.0223\n",
      "Epoch: 008/010 | Batch 200/468 | Cost: 0.1027\n",
      "Epoch: 008/010 | Batch 250/468 | Cost: 0.0027\n",
      "Epoch: 008/010 | Batch 300/468 | Cost: 0.0147\n",
      "Epoch: 008/010 | Batch 350/468 | Cost: 0.0425\n",
      "Epoch: 008/010 | Batch 400/468 | Cost: 0.0126\n",
      "Epoch: 008/010 | Batch 450/468 | Cost: 0.0262\n",
      "Epoch: 008/010 training accuracy: 99.18%\n",
      "Epoch: 009/010 | Batch 000/468 | Cost: 0.0091\n",
      "Epoch: 009/010 | Batch 050/468 | Cost: 0.0151\n",
      "Epoch: 009/010 | Batch 100/468 | Cost: 0.0099\n",
      "Epoch: 009/010 | Batch 150/468 | Cost: 0.0467\n",
      "Epoch: 009/010 | Batch 200/468 | Cost: 0.0069\n",
      "Epoch: 009/010 | Batch 250/468 | Cost: 0.0483\n",
      "Epoch: 009/010 | Batch 300/468 | Cost: 0.0719\n",
      "Epoch: 009/010 | Batch 350/468 | Cost: 0.0298\n",
      "Epoch: 009/010 | Batch 400/468 | Cost: 0.0363\n",
      "Epoch: 009/010 | Batch 450/468 | Cost: 0.0081\n",
      "Epoch: 009/010 training accuracy: 99.39%\n",
      "Epoch: 010/010 | Batch 000/468 | Cost: 0.0338\n",
      "Epoch: 010/010 | Batch 050/468 | Cost: 0.0038\n",
      "Epoch: 010/010 | Batch 100/468 | Cost: 0.0226\n",
      "Epoch: 010/010 | Batch 150/468 | Cost: 0.0187\n",
      "Epoch: 010/010 | Batch 200/468 | Cost: 0.0104\n",
      "Epoch: 010/010 | Batch 250/468 | Cost: 0.0092\n",
      "Epoch: 010/010 | Batch 300/468 | Cost: 0.0819\n",
      "Epoch: 010/010 | Batch 350/468 | Cost: 0.0491\n",
      "Epoch: 010/010 | Batch 400/468 | Cost: 0.0549\n",
      "Epoch: 010/010 | Batch 450/468 | Cost: 0.0020\n",
      "Epoch: 010/010 training accuracy: 99.24%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "            \n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_dataset)//batch_size, cost))\n",
    "\n",
    "    model = model.eval() # eval mode to prevent upd. batchnorm params during inference\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "              epoch+1, num_epochs, \n",
    "              compute_accuracy(model, train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.33%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
